import os
import numpy as np
from PyPDF2 import PdfReader
from sklearn.neighbors import NearestNeighbors
from tqdm import tqdm
from sentence_transformers import SentenceTransformer

# Define path to your PDF files
pdf_dir = r"C:\Users\yagna\OneDrive\Desktop\Task 1"  # Replace this with the actual path to the PDFs

# Ensure the directory exists
if not os.path.exists(pdf_dir):
    raise FileNotFoundError(f"The directory '{pdf_dir}' does not exist. Please provide a valid directory.")
    
# Load Sentence-BERT model for embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to extract text from specific pages of a PDF
def extract_text_from_pdf(pdf_path, pages=[2, 6]):
    text_chunks = []
    reader = PdfReader(pdf_path)
    
    # Check if the pages list is within bounds of the PDF pages
    total_pages = len(reader.pages)
    for page_num in pages:
        if page_num <= total_pages:
            text = reader.pages[page_num - 1].extract_text()  # Page numbers are 1-based
            if text:
                text_chunks.append(text)
    return text_chunks

# Function to chunk text into smaller parts for processing
def chunk_text(text, max_chunk_size=500):
    words = text.split()
    chunks = []
    current_chunk = []
    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= max_chunk_size:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks

# Function to store embeddings in a NearestNeighbors model
def store_embeddings_in_knn(embeddings, metadata):
    knn = NearestNeighbors(n_neighbors=min(5, len(embeddings)), algorithm='auto', metric='cosine')
    knn.fit(embeddings)
    return knn, metadata

# Process PDF files and generate embeddings
all_embeddings = []
metadata = []
for pdf_file in tqdm(os.listdir(pdf_dir)):
    if pdf_file.endswith(".pdf"):
        pdf_path = os.path.join(pdf_dir, pdf_file)
        text_chunks = extract_text_from_pdf(pdf_path)  # Only extract from pages 2 and 6
        for text in text_chunks:
            chunks = chunk_text(text)
            embeddings = model.encode(chunks)  # Use Sentence-BERT for embeddings
            all_embeddings.extend(embeddings)
            metadata.extend([(pdf_file, chunk) for chunk in chunks])

# Check if embeddings are generated correctly
print(f"Total embeddings generated: {len(all_embeddings)}")

if len(all_embeddings) == 0:
    raise ValueError("No embeddings were generated. Please check the PDF content and extraction process.")

# Convert embeddings to numpy array
all_embeddings = np.array(all_embeddings)

# Store embeddings using NearestNeighbors
knn, metadata = store_embeddings_in_knn(all_embeddings, metadata)

# Query Handling function
def query_knn(query, knn, metadata, top_k=5):
    query_embedding = model.encode([query]).reshape(1, -1)
    
    # Dynamically adjust n_neighbors if there are fewer than 5 embeddings
    n_neighbors = min(top_k, len(all_embeddings))
    
    distances, indices = knn.kneighbors(query_embedding, n_neighbors=n_neighbors)
    results = [(metadata[idx], distances[0][i]) for i, idx in enumerate(indices[0])]
    return results

# Example Query 1: Retrieve unemployment information for people with a bachelor's degree
query = "What is the unemployment rate for people with a bachelor's degree?"
results = query_knn(query, knn, metadata)
print(query)
# Display Results
for (file_chunk, score) in results:
    print(f"File: {file_chunk[0]}, Chunk: {file_chunk[1]}, Score: {score}")
print()

# Example Query 2: Retrieve tabular data
query = "Extract the tabular data on economic indicators."
results = query_knn(query, knn, metadata)
print(query)
# Display Results
for (file_chunk, score) in results:
    print(f"File: {file_chunk[0]}, Chunk: {file_chunk[1]}, Score: {score}")
print()

# Example Query 3: Compare unemployment rates between bachelor’s and master’s degrees
query = "Compare the unemployment rate for people with bachelor's and master's degrees."
results = query_knn(query, knn, metadata)
print(query)
# Display Comparison Results
for (file_chunk, score) in results:
    print(f"File: {file_chunk[0]}, Chunk: {file_chunk[1]}, Score: {score}")
    print(query)